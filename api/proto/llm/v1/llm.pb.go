// NeuroGate LLM Service Protocol Buffer Definition
// This defines the gRPC contract between Gateway and Worker nodes

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v6.33.2
// source: api/proto/llm/v1/llm.proto

package llmv1

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// PromptRequest contains the input for text generation
type PromptRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The unique request identifier for tracing
	RequestId string `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	// The prompt text to send to the LLM
	Prompt string `protobuf:"bytes,2,opt,name=prompt,proto3" json:"prompt,omitempty"`
	// The model to use (e.g., "llama3", "mistral")
	Model string `protobuf:"bytes,3,opt,name=model,proto3" json:"model,omitempty"`
	// Maximum number of tokens to generate
	MaxTokens int32 `protobuf:"varint,4,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	// Temperature for sampling (0.0 - 2.0)
	Temperature float32 `protobuf:"fixed32,5,opt,name=temperature,proto3" json:"temperature,omitempty"`
	// Optional system prompt for context
	SystemPrompt  string `protobuf:"bytes,6,opt,name=system_prompt,json=systemPrompt,proto3" json:"system_prompt,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PromptRequest) Reset() {
	*x = PromptRequest{}
	mi := &file_api_proto_llm_v1_llm_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PromptRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PromptRequest) ProtoMessage() {}

func (x *PromptRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_llm_v1_llm_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PromptRequest.ProtoReflect.Descriptor instead.
func (*PromptRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_llm_v1_llm_proto_rawDescGZIP(), []int{0}
}

func (x *PromptRequest) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *PromptRequest) GetPrompt() string {
	if x != nil {
		return x.Prompt
	}
	return ""
}

func (x *PromptRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *PromptRequest) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *PromptRequest) GetTemperature() float32 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *PromptRequest) GetSystemPrompt() string {
	if x != nil {
		return x.SystemPrompt
	}
	return ""
}

// PromptResponse contains the generated text
type PromptResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The unique request identifier (echoed back)
	RequestId string `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	// The generated text response
	Response string `protobuf:"bytes,2,opt,name=response,proto3" json:"response,omitempty"`
	// Number of tokens in the prompt
	PromptTokens int32 `protobuf:"varint,3,opt,name=prompt_tokens,json=promptTokens,proto3" json:"prompt_tokens,omitempty"`
	// Number of tokens generated
	CompletionTokens int32 `protobuf:"varint,4,opt,name=completion_tokens,json=completionTokens,proto3" json:"completion_tokens,omitempty"`
	// Total tokens used
	TotalTokens int32 `protobuf:"varint,5,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	// Time taken for inference in milliseconds
	InferenceTimeMs int64 `protobuf:"varint,6,opt,name=inference_time_ms,json=inferenceTimeMs,proto3" json:"inference_time_ms,omitempty"`
	// The model used for generation
	Model         string `protobuf:"bytes,7,opt,name=model,proto3" json:"model,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PromptResponse) Reset() {
	*x = PromptResponse{}
	mi := &file_api_proto_llm_v1_llm_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PromptResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PromptResponse) ProtoMessage() {}

func (x *PromptResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_llm_v1_llm_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PromptResponse.ProtoReflect.Descriptor instead.
func (*PromptResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_llm_v1_llm_proto_rawDescGZIP(), []int{1}
}

func (x *PromptResponse) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *PromptResponse) GetResponse() string {
	if x != nil {
		return x.Response
	}
	return ""
}

func (x *PromptResponse) GetPromptTokens() int32 {
	if x != nil {
		return x.PromptTokens
	}
	return 0
}

func (x *PromptResponse) GetCompletionTokens() int32 {
	if x != nil {
		return x.CompletionTokens
	}
	return 0
}

func (x *PromptResponse) GetTotalTokens() int32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

func (x *PromptResponse) GetInferenceTimeMs() int64 {
	if x != nil {
		return x.InferenceTimeMs
	}
	return 0
}

func (x *PromptResponse) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

// TokenResponse for streaming responses
type TokenResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The unique request identifier
	RequestId string `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	// A single token or chunk of text
	Token string `protobuf:"bytes,2,opt,name=token,proto3" json:"token,omitempty"`
	// Whether this is the final token
	Done bool `protobuf:"varint,3,opt,name=done,proto3" json:"done,omitempty"`
	// Running count of tokens generated
	TokensGenerated int32 `protobuf:"varint,4,opt,name=tokens_generated,json=tokensGenerated,proto3" json:"tokens_generated,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *TokenResponse) Reset() {
	*x = TokenResponse{}
	mi := &file_api_proto_llm_v1_llm_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TokenResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TokenResponse) ProtoMessage() {}

func (x *TokenResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_llm_v1_llm_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TokenResponse.ProtoReflect.Descriptor instead.
func (*TokenResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_llm_v1_llm_proto_rawDescGZIP(), []int{2}
}

func (x *TokenResponse) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *TokenResponse) GetToken() string {
	if x != nil {
		return x.Token
	}
	return ""
}

func (x *TokenResponse) GetDone() bool {
	if x != nil {
		return x.Done
	}
	return false
}

func (x *TokenResponse) GetTokensGenerated() int32 {
	if x != nil {
		return x.TokensGenerated
	}
	return 0
}

// HealthCheckRequest for worker health verification
type HealthCheckRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional: include a timestamp for latency measurement
	Timestamp     int64 `protobuf:"varint,1,opt,name=timestamp,proto3" json:"timestamp,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *HealthCheckRequest) Reset() {
	*x = HealthCheckRequest{}
	mi := &file_api_proto_llm_v1_llm_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *HealthCheckRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*HealthCheckRequest) ProtoMessage() {}

func (x *HealthCheckRequest) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_llm_v1_llm_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use HealthCheckRequest.ProtoReflect.Descriptor instead.
func (*HealthCheckRequest) Descriptor() ([]byte, []int) {
	return file_api_proto_llm_v1_llm_proto_rawDescGZIP(), []int{3}
}

func (x *HealthCheckRequest) GetTimestamp() int64 {
	if x != nil {
		return x.Timestamp
	}
	return 0
}

// HealthCheckResponse from worker
type HealthCheckResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Whether the worker is healthy and ready
	Healthy bool `protobuf:"varint,1,opt,name=healthy,proto3" json:"healthy,omitempty"`
	// Current load on the worker (0.0 - 1.0)
	Load float32 `protobuf:"fixed32,2,opt,name=load,proto3" json:"load,omitempty"`
	// Number of requests currently being processed
	ActiveRequests int32 `protobuf:"varint,3,opt,name=active_requests,json=activeRequests,proto3" json:"active_requests,omitempty"`
	// Worker version for compatibility checks
	Version string `protobuf:"bytes,4,opt,name=version,proto3" json:"version,omitempty"`
	// Whether Ollama is reachable
	OllamaConnected bool `protobuf:"varint,5,opt,name=ollama_connected,json=ollamaConnected,proto3" json:"ollama_connected,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *HealthCheckResponse) Reset() {
	*x = HealthCheckResponse{}
	mi := &file_api_proto_llm_v1_llm_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *HealthCheckResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*HealthCheckResponse) ProtoMessage() {}

func (x *HealthCheckResponse) ProtoReflect() protoreflect.Message {
	mi := &file_api_proto_llm_v1_llm_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use HealthCheckResponse.ProtoReflect.Descriptor instead.
func (*HealthCheckResponse) Descriptor() ([]byte, []int) {
	return file_api_proto_llm_v1_llm_proto_rawDescGZIP(), []int{4}
}

func (x *HealthCheckResponse) GetHealthy() bool {
	if x != nil {
		return x.Healthy
	}
	return false
}

func (x *HealthCheckResponse) GetLoad() float32 {
	if x != nil {
		return x.Load
	}
	return 0
}

func (x *HealthCheckResponse) GetActiveRequests() int32 {
	if x != nil {
		return x.ActiveRequests
	}
	return 0
}

func (x *HealthCheckResponse) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

func (x *HealthCheckResponse) GetOllamaConnected() bool {
	if x != nil {
		return x.OllamaConnected
	}
	return false
}

var File_api_proto_llm_v1_llm_proto protoreflect.FileDescriptor

const file_api_proto_llm_v1_llm_proto_rawDesc = "" +
	"\n" +
	"\x1aapi/proto/llm/v1/llm.proto\x12\x06llm.v1\"\xc2\x01\n" +
	"\rPromptRequest\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x16\n" +
	"\x06prompt\x18\x02 \x01(\tR\x06prompt\x12\x14\n" +
	"\x05model\x18\x03 \x01(\tR\x05model\x12\x1d\n" +
	"\n" +
	"max_tokens\x18\x04 \x01(\x05R\tmaxTokens\x12 \n" +
	"\vtemperature\x18\x05 \x01(\x02R\vtemperature\x12#\n" +
	"\rsystem_prompt\x18\x06 \x01(\tR\fsystemPrompt\"\x82\x02\n" +
	"\x0ePromptResponse\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x1a\n" +
	"\bresponse\x18\x02 \x01(\tR\bresponse\x12#\n" +
	"\rprompt_tokens\x18\x03 \x01(\x05R\fpromptTokens\x12+\n" +
	"\x11completion_tokens\x18\x04 \x01(\x05R\x10completionTokens\x12!\n" +
	"\ftotal_tokens\x18\x05 \x01(\x05R\vtotalTokens\x12*\n" +
	"\x11inference_time_ms\x18\x06 \x01(\x03R\x0finferenceTimeMs\x12\x14\n" +
	"\x05model\x18\a \x01(\tR\x05model\"\x83\x01\n" +
	"\rTokenResponse\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x14\n" +
	"\x05token\x18\x02 \x01(\tR\x05token\x12\x12\n" +
	"\x04done\x18\x03 \x01(\bR\x04done\x12)\n" +
	"\x10tokens_generated\x18\x04 \x01(\x05R\x0ftokensGenerated\"2\n" +
	"\x12HealthCheckRequest\x12\x1c\n" +
	"\ttimestamp\x18\x01 \x01(\x03R\ttimestamp\"\xb1\x01\n" +
	"\x13HealthCheckResponse\x12\x18\n" +
	"\ahealthy\x18\x01 \x01(\bR\ahealthy\x12\x12\n" +
	"\x04load\x18\x02 \x01(\x02R\x04load\x12'\n" +
	"\x0factive_requests\x18\x03 \x01(\x05R\x0eactiveRequests\x12\x18\n" +
	"\aversion\x18\x04 \x01(\tR\aversion\x12)\n" +
	"\x10ollama_connected\x18\x05 \x01(\bR\x0follamaConnected2\xd9\x01\n" +
	"\n" +
	"LLMService\x12=\n" +
	"\fGenerateText\x12\x15.llm.v1.PromptRequest\x1a\x16.llm.v1.PromptResponse\x12D\n" +
	"\x12StreamGenerateText\x12\x15.llm.v1.PromptRequest\x1a\x15.llm.v1.TokenResponse0\x01\x12F\n" +
	"\vHealthCheck\x12\x1a.llm.v1.HealthCheckRequest\x1a\x1b.llm.v1.HealthCheckResponseB<Z:github.com/hugovillarreal/neurogate/api/proto/llm/v1;llmv1b\x06proto3"

var (
	file_api_proto_llm_v1_llm_proto_rawDescOnce sync.Once
	file_api_proto_llm_v1_llm_proto_rawDescData []byte
)

func file_api_proto_llm_v1_llm_proto_rawDescGZIP() []byte {
	file_api_proto_llm_v1_llm_proto_rawDescOnce.Do(func() {
		file_api_proto_llm_v1_llm_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_api_proto_llm_v1_llm_proto_rawDesc), len(file_api_proto_llm_v1_llm_proto_rawDesc)))
	})
	return file_api_proto_llm_v1_llm_proto_rawDescData
}

var file_api_proto_llm_v1_llm_proto_msgTypes = make([]protoimpl.MessageInfo, 5)
var file_api_proto_llm_v1_llm_proto_goTypes = []any{
	(*PromptRequest)(nil),       // 0: llm.v1.PromptRequest
	(*PromptResponse)(nil),      // 1: llm.v1.PromptResponse
	(*TokenResponse)(nil),       // 2: llm.v1.TokenResponse
	(*HealthCheckRequest)(nil),  // 3: llm.v1.HealthCheckRequest
	(*HealthCheckResponse)(nil), // 4: llm.v1.HealthCheckResponse
}
var file_api_proto_llm_v1_llm_proto_depIdxs = []int32{
	0, // 0: llm.v1.LLMService.GenerateText:input_type -> llm.v1.PromptRequest
	0, // 1: llm.v1.LLMService.StreamGenerateText:input_type -> llm.v1.PromptRequest
	3, // 2: llm.v1.LLMService.HealthCheck:input_type -> llm.v1.HealthCheckRequest
	1, // 3: llm.v1.LLMService.GenerateText:output_type -> llm.v1.PromptResponse
	2, // 4: llm.v1.LLMService.StreamGenerateText:output_type -> llm.v1.TokenResponse
	4, // 5: llm.v1.LLMService.HealthCheck:output_type -> llm.v1.HealthCheckResponse
	3, // [3:6] is the sub-list for method output_type
	0, // [0:3] is the sub-list for method input_type
	0, // [0:0] is the sub-list for extension type_name
	0, // [0:0] is the sub-list for extension extendee
	0, // [0:0] is the sub-list for field type_name
}

func init() { file_api_proto_llm_v1_llm_proto_init() }
func file_api_proto_llm_v1_llm_proto_init() {
	if File_api_proto_llm_v1_llm_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_api_proto_llm_v1_llm_proto_rawDesc), len(file_api_proto_llm_v1_llm_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   5,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_api_proto_llm_v1_llm_proto_goTypes,
		DependencyIndexes: file_api_proto_llm_v1_llm_proto_depIdxs,
		MessageInfos:      file_api_proto_llm_v1_llm_proto_msgTypes,
	}.Build()
	File_api_proto_llm_v1_llm_proto = out.File
	file_api_proto_llm_v1_llm_proto_goTypes = nil
	file_api_proto_llm_v1_llm_proto_depIdxs = nil
}
